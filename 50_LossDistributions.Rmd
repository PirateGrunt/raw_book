# Loss Distributions

By the end of this chapter, you will know the following:

* Simulation with `base` functions
* How to perform basic visualization of loss data
* How to fit a loss distributions
* Goodness of fit

### Packages we'll use
  
* `MASS` (MASS = Modern Applied Statistics in S)
    * `fitdistr` will fit a distribution to a loss distribution function
* `actuar`
    * `emm` calculates empirical moments
    * `lev` limited expected value
    * `coverage` modifies a loss distribution for coverage elements
    * Contains many more distributions than are found in `base` R such as Burr, Pareto, etc. Basically, anything in "Loss Models" is likely to be found here.
    * Contains the dental claims data from "Loss Models"
* Direct optimization
    * `optim` function

### Statistical distributions in R

Function names are one of 'd', 'p', 'q', 'r' + function name

* d - probability density
* p - cumulative distribution function
* q - quantiles
* r - random number generator

### Examples

```{r }
mu <- 10000
CV <- 0.30
sd <- mu * CV
x <- seq(mu - sd*3, mu + sd * 3, length.out = 20)
p <- seq(.05, .95, by = .05)

dnorm(x, mu, sd)
pnorm(x, mu, sd)
qnorm(p, mu, sd)
rnorm(10, mu, sd)

dlnorm(x, log(mu), log(sd))
plnorm(x, log(mu), log(sd))

plot(function(x) {dnorm(x, 10, 4)}, 0, 20)
```

### Generate some loss data

```{r }
set.seed(8910)
years <- 2001:2010
frequency <- 1000

N <- rpois(length(years), frequency)

sevShape <- 2
sevScale <- 1000
severity <- rgamma(sum(N), sevShape, scale = sevScale)

summary(severity)
```

### Histograms

```{r}
hist(severity)
hist(severity, breaks = 50)

hist(log(severity), breaks = 50)
```

### Density

The kernel density is effectively a smoothed histogram.

```{r}
plot(density(severity))

plot(density(log(severity)))
```

###`fitdistr`

```{r }
library(MASS)

fitGamma <- fitdistr(severity, "gamma")
fitLognormal <- fitdistr(severity, "lognormal")
fitWeibull <- fitdistr(severity, "Weibull")

fitGamma
fitLognormal
fitWeibull
```

###q-q plot

```{r }
probabilities = (1:(sum(N)))/(sum(N)+1)

weibullQ <- qweibull(probabilities, coef(fitWeibull)[1], coef(fitWeibull)[2])
lnQ <- qlnorm(probabilities, coef(fitLognormal)[1], coef(fitLognormal)[2])
gammaQ <- qgamma(probabilities, coef(fitGamma)[1], coef(fitGamma)[2])

sortedSeverity <- sort(severity)
oldPar <- par(mfrow = c(1,3))
plot(sort(weibullQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Weibull Fit")
abline(0,1)

plot(sort(lnQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Lognormal Fit")
abline(0,1)

plot(sort(gammaQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Gamma Fit")
abline(0,1)

par(oldPar)
```

###Compare fit to histogram

```{r }
sampleLogMean <- fitLognormal$estimate[1]
sampleLogSd <- fitLognormal$estimate[2]

sampleShape <- fitGamma$estimate[1]
sampleRate <- fitGamma$estimate[2]

sampleShapeW <- fitWeibull$estimate[1]
sampleScaleW <- fitWeibull$estimate[2]

x <- seq(0, max(severity), length.out=500)
yLN <- dlnorm(x, sampleLogMean, sampleLogSd)
yGamma <- dgamma(x, sampleShape, sampleRate)
yWeibull <- dweibull(x, sampleShapeW, sampleScaleW)

hist(severity, freq=FALSE, ylim=range(yLN, yGamma))

lines(x, yLN, col="blue")
lines(x, yGamma, col="red")
lines(x, yWeibull, col="green")
```

###Kolmogorov-Smirnov

The Kolmogorov-Smirnov test measures the distance between an sample distribution and a candidate loss distribution. More formal than q-q plots. 

```{r}
sampleCumul <- seq(1, length(severity)) / length(severity)
stepSample  <- stepfun(sortedSeverity, c(0, sampleCumul), f = 0)
yGamma <- pgamma(sortedSeverity, sampleShape, sampleRate)
yWeibull <- pweibull(sortedSeverity, sampleShapeW, sampleScaleW)
yLN <- plnorm(sortedSeverity, sampleLogMean, sampleLogSd)

plot(stepSample, col="black", main = "K-S Gamma")
lines(sortedSeverity, yGamma, col = "blue")

plot(stepSample, col="black", main = "K-S Weibull")
lines(sortedSeverity, yWeibull, col = "blue")

plot(stepSample, col="black", main = "K-S Lognormal")
lines(sortedSeverity, yLN, col = "blue")
```

###More K-S

A low value for D indicates that the selected curve is fairly close to our data. The p-value indicates the chance that D was produced by the null hypothesis.

```{r }
testGamma <- ks.test(severity, "pgamma", sampleShape, sampleRate)
testLN <- ks.test(severity, "plnorm", sampleLogMean, sampleLogSd)
testWeibull <- ks.test(severity, "pweibull", sampleShapeW, sampleScaleW)

testGamma
testLN
testWeibull
```

###Direct optimization

The `optim` function will optimize a function. Works very similar to the Solver algorithm in Excel. `optim` takes a function as an argument, so let's create a function.

```{r}
quadraticFun <- function(a, b, c){
  function(x) a*x^2 + b*x + c
}

myQuad <- quadraticFun(a=4, b=-3, c=3)
plot(myQuad, -10, 10)
```

### Direct optimization

8 is our initial guess. A good initial guess will speed up conversion.

```{r }
myResult <- optim(8, myQuad)
myResult
```

## Direct optimization
Default is to minimize. Set the parameter `fnscale` to something negative to convert to a maximization problem.

```{r }
myOtherQuad <- quadraticFun(-6, 20, -5)
plot(myOtherQuad, -10, 10)
myResult <- optim(8, myOtherQuad)
myResult <- optim(8, myOtherQuad, control = list(fnscale=-1))
```

###Direct optimization

Direct optimization allows us to create another objective function to maximize, or work with loss distributions for which there isn't yet support in a package like `actuar`. May be used for general purpose optimization problems, e.g. maximize rate of return for various capital allocation methods.

Note that optimization is a general, solved problem. Things like the simplex method already have package solutions in R. You don't need to reinvent the wheel!

###Questions

* Plot a lognormal distribution with a mean of $10,000 and a CV of 30%.
* For that distribution, what is the probability of seeing a claim greater than $100,000?
* Generate 100 and 1,000 observations from that distribution.
* Draw a histogram for each sample.
* What are the mean, standard deviation and CV of each sample?
* Convince yourself that the sample data were not produced by a Weibull distribution.
* Assuming that losses are Poisson distributed, with expected value of 200, estimate the aggregate loss distribution.
* What is the cost of a $50,000 xs $50,000 layer of reinsurance?

###Answers

```{r }
severity <- 10000
CV <- .3
sigma <- sqrt(log(1 + CV^2))
mu <- log(severity) - sigma^2/2
plot(function(x) dlnorm(x), mu, sigma, ylab="LN f(x)")
```

<!-- ## Basic question

    What is the cost of a 1 million xs 1 million layer?
    
A few other questions:

* How many times has the layer been hit?
* What is the average cost when it has been?
* What is the variance of losses in the layer?-->